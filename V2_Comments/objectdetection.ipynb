{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a055c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset : https://www.kaggle.com/datasets/imbikramsaha/caltech-101\n",
    "# model link : https://www.kaggle.com/datasets/keras/vgg16/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dce5684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a4047d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "546dbf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9144 images belonging to 102 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"./caltech-101-img\" #Specifies the directory path where the dataset is located\n",
    "dataset_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    ")\n",
    "#normalises the image\n",
    "\n",
    "# # here batch_size is the number of images in each batch\n",
    "batch_size = 2000\n",
    "dataset_generator = dataset_datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(64, 64), #resizes the image into 64 by 64 pixel\n",
    "    batch_size=batch_size, #Sets the batch size for training.\n",
    "    class_mode='categorical' # labels are one-hot encoded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22f7f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train =  dataset_generator[0]\n",
    "x_test, y_test = dataset_generator[1]\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76a5878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c599aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b9b2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "base_model = VGG16(weights=weights_path, include_top=False, input_shape=(64, 64, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0892cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "   layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b25a0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(base_model.output)\n",
    "# Explanation: This line adds a Flatten layer to the output of the base_model. The Flatten layer is used to transform the 3D tensor output from the convolutional base (which is usually the output of the last convolutional layer) into a 1D tensor. This flattening step is necessary when transitioning from convolutional layers to densely connected layers.\n",
    "# Example: Suppose the output shape of base_model is (7, 7, 512). This means you have a 3D tensor with dimensions 7x7x512. Applying the Flatten layer converts this 3D tensor into a 1D tensor by unraveling the values along the dimensions. In this case, the resulting 1D tensor would have a size of 7 * 7 * 512 = 25088.\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(102, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba0a99c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.1058 - loss: 4.3117 - val_accuracy: 0.2710 - val_loss: 3.5213\n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1s/step - accuracy: 0.3123 - loss: 3.2023 - val_accuracy: 0.3680 - val_loss: 3.0913\n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1s/step - accuracy: 0.4067 - loss: 2.8013 - val_accuracy: 0.4035 - val_loss: 2.8377\n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1s/step - accuracy: 0.4864 - loss: 2.3627 - val_accuracy: 0.4585 - val_loss: 2.6010\n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.5597 - loss: 2.0470 - val_accuracy: 0.4730 - val_loss: 2.4221\n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1s/step - accuracy: 0.6056 - loss: 1.7890 - val_accuracy: 0.4905 - val_loss: 2.2840\n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1s/step - accuracy: 0.6560 - loss: 1.5851 - val_accuracy: 0.5120 - val_loss: 2.1808\n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.6764 - loss: 1.4371 - val_accuracy: 0.5185 - val_loss: 2.1010\n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1s/step - accuracy: 0.7315 - loss: 1.2629 - val_accuracy: 0.5260 - val_loss: 2.0413\n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.7616 - loss: 1.1338 - val_accuracy: 0.5465 - val_loss: 1.9932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c5cb0dffb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd74f9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.2632 - loss: 3.6391 - val_accuracy: 0.4895 - val_loss: 2.3887\n",
      "Epoch 2/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.5557 - loss: 1.9392 - val_accuracy: 0.5490 - val_loss: 1.9809\n",
      "Epoch 3/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.6955 - loss: 1.2457 - val_accuracy: 0.5510 - val_loss: 1.8548\n",
      "Epoch 4/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.8029 - loss: 0.7549 - val_accuracy: 0.5900 - val_loss: 1.7445\n",
      "Epoch 5/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.8694 - loss: 0.4785 - val_accuracy: 0.6070 - val_loss: 1.7749\n",
      "Epoch 6/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9276 - loss: 0.2980 - val_accuracy: 0.6150 - val_loss: 1.7782\n",
      "Epoch 7/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9546 - loss: 0.1756 - val_accuracy: 0.6130 - val_loss: 1.8904\n",
      "Epoch 8/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9756 - loss: 0.1084 - val_accuracy: 0.6205 - val_loss: 1.8738\n",
      "Epoch 9/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9893 - loss: 0.0686 - val_accuracy: 0.6270 - val_loss: 1.9536\n",
      "Epoch 10/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9856 - loss: 0.0596 - val_accuracy: 0.6240 - val_loss: 1.9925\n",
      "Epoch 11/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9889 - loss: 0.0452 - val_accuracy: 0.6325 - val_loss: 1.9176\n",
      "Epoch 12/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.9929 - loss: 0.0305 - val_accuracy: 0.6390 - val_loss: 2.0289\n",
      "Epoch 13/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9951 - loss: 0.0237 - val_accuracy: 0.6350 - val_loss: 2.0422\n",
      "Epoch 14/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9940 - loss: 0.0307 - val_accuracy: 0.6205 - val_loss: 2.1340\n",
      "Epoch 15/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9962 - loss: 0.0160 - val_accuracy: 0.6385 - val_loss: 2.0541\n",
      "Epoch 16/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9979 - loss: 0.0135 - val_accuracy: 0.6360 - val_loss: 2.0595\n",
      "Epoch 17/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0073 - val_accuracy: 0.6415 - val_loss: 2.0248\n",
      "Epoch 18/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.6460 - val_loss: 2.0003\n",
      "Epoch 19/20\n",
      "\u001b[1m19/32\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 702ms/step - accuracy: 1.0000 - loss: 0.0031"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(weights=weights_path, include_top=False, input_shape=(64, 64, 3))\n",
    "# freeze all layers first\n",
    "for layer in base_model.layers:\n",
    "   layer.trainable = False\n",
    "# unfreeze last 4 layers of base model\n",
    "for layer in base_model.layers[len(base_model.layers) - 2:]:\n",
    "   layer.trainable = True\n",
    "# fine-tuning hyper parameters\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "predictions = Dense(102, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# training fine tuned model\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a09850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "predicted_value = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91552a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(dataset_generator.class_indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b877967",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 887\n",
    "\n",
    "plt.imshow(x_test[n])\n",
    "print(\"Preditcted: \",labels[np.argmax(predicted_value[n])])\n",
    "print(\"Actual: \", labels[np.argmax(y_test[n])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a706e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLet\\'s go through each code snippet in detail, breaking down its function, significance, and possible viva questions with answers.\\n\\n---\\n\\n### 1. Importing Libraries\\n\\n```python\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nimport numpy as np\\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\\n```\\n\\n- **Explanation**: Imports necessary libraries. `tensorflow` for deep learning, `numpy` for numerical operations, and `ImageDataGenerator` for image preprocessing and augmentation.\\n\\n- **Significance**: Importing these libraries enables image preprocessing and model building with TensorFlow.\\n\\n- **Viva Questions**:\\n    1. **Q**: Why do we use `ImageDataGenerator`?  \\n       **A**: `ImageDataGenerator` allows for data augmentation and preprocessing, such as scaling images, which can improve model generalization.\\n    \\n    2. **Q**: What is the purpose of `numpy` in this code?  \\n       **A**: `numpy` provides support for handling numerical operations, such as creating arrays and manipulating numerical data structures.\\n\\n---\\n\\n### 2. Image Data Preparation\\n\\n```python\\ndataset_dir = \"./caltech-101-img\"\\ndataset_datagen = ImageDataGenerator(rescale=1.0 / 255)\\nbatch_size = 2000\\ndataset_generator = dataset_datagen.flow_from_directory(\\n    dataset_dir,\\n    target_size=(64, 64),\\n    batch_size=batch_size,\\n    class_mode=\\'categorical\\'\\n)\\n```\\n\\n- **Explanation**:\\n    - `dataset_dir`: Specifies the directory path where the dataset images are stored.\\n    - `ImageDataGenerator`: Rescales the images to have pixel values between 0 and 1 by dividing by 255.\\n    - `flow_from_directory`: Loads images, resizes them to 64x64 pixels, and sets batch size to 2000 with categorical labels (one-hot encoded).\\n\\n- **Significance**: Prepares image data by normalizing and resizing for compatibility with the model.\\n\\n- **Viva Questions**:\\n    1. **Q**: Why do we rescale the images by dividing by 255?  \\n       **A**: Rescaling normalizes pixel values to the range [0, 1], which speeds up training and improves performance.\\n\\n    2. **Q**: What is the purpose of `target_size`?  \\n       **A**: `target_size` resizes all images to 64x64 pixels, ensuring a consistent input shape for the model.\\n\\n---\\n\\n### 3. Splitting the Dataset\\n\\n```python\\nx_train, y_train = dataset_generator[0]\\nx_test, y_test = dataset_generator[1]\\n```\\n\\n- **Explanation**: Retrieves the first and second batches from `dataset_generator` as `x_train`/`y_train` and `x_test`/`y_test`.\\n\\n- **Significance**: Divides the data into training and testing sets for model evaluation.\\n\\n- **Viva Questions**:\\n    1. **Q**: Why do we use `dataset_generator[0]` and `dataset_generator[1]`?  \\n       **A**: This loads the first two batches from the generator, serving as training and testing data.\\n\\n---\\n\\n### 4. Creating the Base Model\\n\\n```python\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.layers import Dense, Flatten\\nfrom tensorflow.keras.applications import VGG16\\n\\nweights_path = \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\\nbase_model = VGG16(weights=weights_path, include_top=False, input_shape=(64, 64, 3))\\nfor layer in base_model.layers:\\n   layer.trainable = False\\n```\\n\\n- **Explanation**:\\n    - Loads the pre-trained VGG16 model without the top classification layer.\\n    - Freezes the weights in all layers to prevent updating during training.\\n\\n- **Significance**: Uses transfer learning from VGG16, which has learned general features that improve training efficiency.\\n\\n- **Viva Questions**:\\n    1. **Q**: What is the advantage of setting `include_top=False` in VGG16?  \\n       **A**: It allows us to exclude VGG16’s dense layers so we can add custom layers suitable for our dataset.\\n\\n    2. **Q**: Why do we freeze the layers in `base_model`?  \\n       **A**: Freezing preserves learned features, allowing the model to focus on learning task-specific features in new layers.\\n\\n---\\n\\n### 5. Adding Layers to the Model\\n\\n```python\\nx = Flatten()(base_model.output)\\nx = Dense(64, activation=\\'relu\\')(x)\\npredictions = Dense(102, activation=\\'softmax\\')(x)\\n\\nmodel = Model(inputs=base_model.input, outputs=predictions)\\nmodel.compile(optimizer=\"adam\", loss=\\'categorical_crossentropy\\', metrics=[\\'accuracy\\'])\\n```\\n\\n- **Explanation**:\\n    - **Flatten**: Converts 3D output of VGG16 into a 1D tensor.\\n    - **Dense(64)**: Adds a fully connected layer with 64 neurons.\\n    - **Dense(102)**: Adds output layer with 102 classes, using `softmax` for multiclass classification.\\n\\n- **Significance**: Adds classification layers on top of the pre-trained base model, customizing it for the current dataset.\\n\\n- **Viva Questions**:\\n    1. **Q**: Why is the `Flatten` layer added here?  \\n       **A**: `Flatten` changes the 3D output into a 1D array, making it compatible with dense layers.\\n\\n    2. **Q**: Why do we use `softmax` in the final layer?  \\n       **A**: `softmax` is used for multiclass classification, outputting a probability distribution over 102 classes.\\n\\n---\\n\\n### 6. Training the Model\\n\\n```python\\nmodel.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))\\n```\\n\\n- **Explanation**: Trains the model on the dataset for 10 epochs with a batch size of 64, using `x_test`/`y_test` for validation.\\n\\n- **Significance**: Helps optimize model weights for accurate classification.\\n\\n- **Viva Questions**:\\n    1. **Q**: What is the purpose of using `validation_data` during training?  \\n       **A**: `validation_data` helps monitor the model\\'s performance on unseen data to prevent overfitting.\\n\\n---\\n\\n### 7. Fine-tuning the Model\\n\\n```python\\nfor layer in base_model.layers[len(base_model.layers) - 2:]:\\n   layer.trainable = True\\nx = Flatten()(base_model.output)\\nx = Dense(512, activation=\\'relu\\')(x)\\nx = tf.keras.layers.Dropout(0.3)(x)\\npredictions = Dense(102, activation=\\'softmax\\')(x)\\nmodel = Model(inputs=base_model.input, outputs=predictions)\\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss=\\'categorical_crossentropy\\', metrics=[\\'accuracy\\'])\\nmodel.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=(x_test, y_test))\\n```\\n\\n- **Explanation**:\\n    - **Fine-tunes**: Unfreezes the last two layers of VGG16 to allow adaptation to the new dataset.\\n    - **Dropout(0.3)**: Adds dropout regularization to reduce overfitting.\\n\\n- **Significance**: Fine-tuning improves feature extraction for the specific dataset while preventing overfitting.\\n\\n- **Viva Questions**:\\n    1. **Q**: What is the purpose of adding dropout?  \\n       **A**: Dropout reduces overfitting by randomly disabling neurons during training.\\n\\n    2. **Q**: Why are only the last few layers of VGG16 unfrozen?  \\n       **A**: Unfreezing specific layers allows the model to learn dataset-specific patterns without altering core features.\\n\\n---\\n\\n### 8. Making Predictions and Displaying Results\\n\\n```python\\nimport matplotlib.pyplot as plt\\npredicted_value = model.predict(x_test)\\nlabels = list(dataset_generator.class_indices.keys())\\nn = 887\\n\\nplt.imshow(x_test[n])\\nprint(\"Predicted: \", labels[np.argmax(predicted_value[n])])\\nprint(\"Actual: \", labels[np.argmax(y_test[n])])\\n```\\n\\n- **Explanation**:\\n    - Uses `model.predict` to get predicted classes for test images.\\n    - Displays the predicted and actual label for an example image.\\n\\n- **Significance**: Allows visual comparison of model predictions with ground truth, which is essential for assessing model performance.\\n\\n- **Viva Questions**:\\n    1. **Q**: How does `model.predict` work?  \\n       **A**: `model.predict` takes input data and outputs the predicted class probabilities.\\n\\n    2. **Q**: Why do we use `np.argmax` in the print statement?  \\n       **A**: `np.argmax` finds the index of the maximum value, indicating the predicted class in a one-hot encoded array.\\n\\n---\\n\\nThis comprehensive guide should prepare you well for viva questions by explaining the functionality, significance, and typical questions for each code snippet in your project. Let me know if you need more details on specific parts!\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Let's go through each code snippet in detail, breaking down its function, significance, and possible viva questions with answers.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Importing Libraries\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "```\n",
    "\n",
    "- **Explanation**: Imports necessary libraries. `tensorflow` for deep learning, `numpy` for numerical operations, and `ImageDataGenerator` for image preprocessing and augmentation.\n",
    "\n",
    "- **Significance**: Importing these libraries enables image preprocessing and model building with TensorFlow.\n",
    "\n",
    "- **Viva Questions**:\n",
    "    1. **Q**: Why do we use `ImageDataGenerator`?  \n",
    "       **A**: `ImageDataGenerator` allows for data augmentation and preprocessing, such as scaling images, which can improve model generalization.\n",
    "    \n",
    "    2. **Q**: What is the purpose of `numpy` in this code?  \n",
    "       **A**: `numpy` provides support for handling numerical operations, such as creating arrays and manipulating numerical data structures.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Image Data Preparation\n",
    "\n",
    "```python\n",
    "dataset_dir = \"./caltech-101-img\"\n",
    "dataset_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "batch_size = 2000\n",
    "dataset_generator = dataset_datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "```\n",
    "\n",
    "- **Explanation**:\n",
    "    - `dataset_dir`: Specifies the directory path where the dataset images are stored.\n",
    "    - `ImageDataGenerator`: Rescales the images to have pixel values between 0 and 1 by dividing by 255.\n",
    "    - `flow_from_directory`: Loads images, resizes them to 64x64 pixels, and sets batch size to 2000 with categorical labels (one-hot encoded).\n",
    "\n",
    "- **Significance**: Prepares image data by normalizing and resizing for compatibility with the model.\n",
    "\n",
    "- **Viva Questions**:\n",
    "    1. **Q**: Why do we rescale the images by dividing by 255?  \n",
    "       **A**: Rescaling normalizes pixel values to the range [0, 1], which speeds up training and improves performance.\n",
    "\n",
    "    2. **Q**: What is the purpose of `target_size`?  \n",
    "       **A**: `target_size` resizes all images to 64x64 pixels, ensuring a consistent input shape for the model.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Splitting the Dataset\n",
    "\n",
    "```python\n",
    "x_train, y_train = dataset_generator[0]\n",
    "x_test, y_test = dataset_generator[1]\n",
    "```\n",
    "\n",
    "- **Explanation**: Retrieves the first and second batches from `dataset_generator` as `x_train`/`y_train` and `x_test`/`y_test`.\n",
    "\n",
    "- **Significance**: Divides the data into training and testing sets for model evaluation.\n",
    "\n",
    "- **Viva Questions**:\n",
    "    1. **Q**: Why do we use `dataset_generator[0]` and `dataset_generator[1]`?  \n",
    "       **A**: This loads the first two batches from the generator, serving as training and testing data.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Creating the Base Model\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "weights_path = \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "base_model = VGG16(weights=weights_path, include_top=False, input_shape=(64, 64, 3))\n",
    "for layer in base_model.layers:\n",
    "   layer.trainable = False\n",
    "```\n",
    "\n",
    "- **Explanation**:\n",
    "    - Loads the pre-trained VGG16 model without the top classification layer.\n",
    "    - Freezes the weights in all layers to prevent updating during training.\n",
    "\n",
    "- **Significance**: Uses transfer learning from VGG16, which has learned general features that improve training efficiency.\n",
    "\n",
    "- **Viva Questions**:\n",
    "    1. **Q**: What is the advantage of setting `include_top=False` in VGG16?  \n",
    "       **A**: It allows us to exclude VGG16’s dense layers so we can add custom layers suitable for our dataset.\n",
    "\n",
    "    2. **Q**: Why do we freeze the layers in `base_model`?  \n",
    "       **A**: Freezing preserves learned features, allowing the model to focus on learning task-specific features in new layers.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Adding Layers to the Model\n",
    "\n",
    "```python\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(102, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "- **Explanation**:\n",
    "    - **Flatten**: Converts 3D output of VGG16 into a 1D tensor.\n",
    "    - **Dense(64)**: Adds a fully connected layer with 64 neurons.\n",
    "    - **Dense(102)**: Adds output layer with 102 classes, using `softmax` for multiclass classification.\n",
    "\n",
    "- **Significance**: Adds classification layers on top of the pre-trained base model, customizing it for the current dataset.\n",
    "\n",
    "- **Viva Questions**:\n",
    "    1. **Q**: Why is the `Flatten` layer added here?  \n",
    "       **A**: `Flatten` changes the 3D output into a 1D array, making it compatible with dense layers.\n",
    "\n",
    "    2. **Q**: Why do we use `softmax` in the final layer?  \n",
    "       **A**: `softmax` is used for multiclass classification, outputting a probability distribution over 102 classes.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Training the Model\n",
    "\n",
    "```python\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))\n",
    "```\n",
    "\n",
    "- **Explanation**: Trains the model on the dataset for 10 epochs with a batch size of 64, using `x_test`/`y_test` for validation.\n",
    "\n",
    "- **Significance**: Helps optimize model weights for accurate classification.\n",
    "\n",
    "- **Viva Questions**:\n",
    "    1. **Q**: What is the purpose of using `validation_data` during training?  \n",
    "       **A**: `validation_data` helps monitor the model's performance on unseen data to prevent overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Fine-tuning the Model\n",
    "\n",
    "```python\n",
    "for layer in base_model.layers[len(base_model.layers) - 2:]:\n",
    "   layer.trainable = True\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "predictions = Dense(102, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=(x_test, y_test))\n",
    "```\n",
    "\n",
    "- **Explanation**:\n",
    "    - **Fine-tunes**: Unfreezes the last two layers of VGG16 to allow adaptation to the new dataset.\n",
    "    - **Dropout(0.3)**: Adds dropout regularization to reduce overfitting.\n",
    "\n",
    "- **Significance**: Fine-tuning improves feature extraction for the specific dataset while preventing overfitting.\n",
    "\n",
    "- **Viva Questions**:\n",
    "    1. **Q**: What is the purpose of adding dropout?  \n",
    "       **A**: Dropout reduces overfitting by randomly disabling neurons during training.\n",
    "\n",
    "    2. **Q**: Why are only the last few layers of VGG16 unfrozen?  \n",
    "       **A**: Unfreezing specific layers allows the model to learn dataset-specific patterns without altering core features.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Making Predictions and Displaying Results\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "predicted_value = model.predict(x_test)\n",
    "labels = list(dataset_generator.class_indices.keys())\n",
    "n = 887\n",
    "\n",
    "plt.imshow(x_test[n])\n",
    "print(\"Predicted: \", labels[np.argmax(predicted_value[n])])\n",
    "print(\"Actual: \", labels[np.argmax(y_test[n])])\n",
    "```\n",
    "\n",
    "- **Explanation**:\n",
    "    - Uses `model.predict` to get predicted classes for test images.\n",
    "    - Displays the predicted and actual label for an example image.\n",
    "\n",
    "- **Significance**: Allows visual comparison of model predictions with ground truth, which is essential for assessing model performance.\n",
    "\n",
    "- **Viva Questions**:\n",
    "    1. **Q**: How does `model.predict` work?  \n",
    "       **A**: `model.predict` takes input data and outputs the predicted class probabilities.\n",
    "\n",
    "    2. **Q**: Why do we use `np.argmax` in the print statement?  \n",
    "       **A**: `np.argmax` finds the index of the maximum value, indicating the predicted class in a one-hot encoded array.\n",
    "\n",
    "---\n",
    "\n",
    "This comprehensive guide should prepare you well for viva questions by explaining the functionality, significance, and typical questions for each code snippet in your project. Let me know if you need more details on specific parts!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88752dfd-c750-40cf-8917-344473391ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
