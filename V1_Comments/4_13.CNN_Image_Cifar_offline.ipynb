{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Loading and preprocessing the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "cifar_train=pd.read_csv('./train_data.csv')\n",
    "x_train=cifar_train.drop('label',axis=1).values\n",
    "y_train=cifar_train['label'].values\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "cifar_test=pd.read_csv('./test_data.csv')\n",
    "x_test=cifar_test.drop('label',axis=1).values\n",
    "y_test=cifar_test['label'].values\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train=x_train/255.0\n",
    "x_test=x_test/255.0\n",
    "\n",
    "x_train=x_train.reshape((-1,32,32,3))\n",
    "x_test=x_test.reshape((-1,32,32,3))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Defining the model's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmwad\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model=models.Sequential([\n",
    "    layers.Conv2D(32,(3,3),activation='relu',input_shape=(32,32,3)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64,(3,3),activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(128,(3,3),activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128,activation='relu'),\n",
    "    layers.Dense(10,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 14ms/step - accuracy: 0.3595 - loss: 1.7358 - val_accuracy: 0.5692 - val_loss: 1.2070\n",
      "Epoch 2/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 14ms/step - accuracy: 0.5914 - loss: 1.1481 - val_accuracy: 0.6338 - val_loss: 1.0454\n",
      "Epoch 3/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 13ms/step - accuracy: 0.6589 - loss: 0.9719 - val_accuracy: 0.6760 - val_loss: 0.9432\n",
      "Epoch 4/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 13ms/step - accuracy: 0.6985 - loss: 0.8502 - val_accuracy: 0.7005 - val_loss: 0.8666\n",
      "Epoch 5/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.7342 - loss: 0.7519 - val_accuracy: 0.7109 - val_loss: 0.8413\n",
      "Epoch 6/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.7606 - loss: 0.6833 - val_accuracy: 0.7094 - val_loss: 0.8642\n",
      "Epoch 7/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.7807 - loss: 0.6272 - val_accuracy: 0.7123 - val_loss: 0.8419\n",
      "Epoch 8/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.7993 - loss: 0.5765 - val_accuracy: 0.6961 - val_loss: 0.9475\n",
      "Epoch 9/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.8165 - loss: 0.5205 - val_accuracy: 0.7199 - val_loss: 0.8603\n",
      "Epoch 10/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.8321 - loss: 0.4759 - val_accuracy: 0.7204 - val_loss: 0.9010\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Estimating the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m201/313\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7315 - loss: 0.8635"
     ]
    }
   ],
   "source": [
    "test_loss,test_acc=model.evaluate(x_test,y_test)\n",
    "print(f'Test accuracy:{test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "n=2\n",
    "plt.imshow(x_test[n].reshape(32,32,3))\n",
    "\n",
    "predictions=model.predict(x_test)\n",
    "\n",
    "print(\"Actual Label: \",classes[np.argmax(y_test[n])])\n",
    "print(\"Predicted Label: \", classes[np.argmax(predictions[n])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here’s a line-by-line explanation of the code with the significance of each word and its value for viva preparation:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Importing Libraries**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "```\n",
    "\n",
    "- **`import pandas as pd`**: Imports the Pandas library and gives it an alias `pd`. Pandas is used for data \n",
    "manipulation and analysis, especially for handling structured data like CSV files.\n",
    "- **`import numpy as np`**: Imports the NumPy library and assigns it an alias `np`. NumPy is used for numerical computations and handling arrays.\n",
    "- **`import matplotlib.pyplot as plt`**: Imports Matplotlib for data visualization. `pyplot` is a submodule for plotting graphs.\n",
    "- **`import tensorflow as tf`**: Imports TensorFlow, a deep learning framework. `tf` is the alias used for simplicity.\n",
    "- **`from tensorflow.keras import layers, models`**: Imports `layers` and `models` from Keras (a high-level API in \n",
    "TensorFlow for building neural networks). `layers` provides various layer types, and `models` is used to create and manage the model.\n",
    "- **`from tensorflow.keras.utils import to_categorical`**: Imports `to_categorical` from Keras utils, used for converting \n",
    "labels into one-hot encoded format.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Loading the Training Data**\n",
    "\n",
    "```python\n",
    "cifar_train = pd.read_csv('./train_data.csv')\n",
    "```\n",
    "\n",
    "- **`pd.read_csv('./train_data.csv')`**: Loads the CIFAR-10 training dataset from the CSV file `train_data.csv` \n",
    "into a Pandas DataFrame called `cifar_train`. The file contains feature data and labels.\n",
    "\n",
    "```python\n",
    "x_train = cifar_train.drop('label', axis=1).values\n",
    "```\n",
    "\n",
    "- **`cifar_train.drop('label', axis=1)`**: Drops the `label` column from the DataFrame, keeping only the feature data.\n",
    "- **`axis=1`**: Specifies that the operation is performed on columns (not rows).\n",
    "- **`.values`**: Converts the DataFrame to a NumPy array, which is the format needed for TensorFlow processing.\n",
    "\n",
    "```python\n",
    "y_train = cifar_train['label'].values\n",
    "```\n",
    "\n",
    "- **`cifar_train['label']`**: Extracts the `label` column from the DataFrame, which contains the target labels (classes).\n",
    "- **`.values`**: Converts the labels into a NumPy array.\n",
    "\n",
    "```python\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "```\n",
    "\n",
    "- **`x_train.shape`**: Prints the shape (dimensions) of the training feature data.\n",
    "- **`y_train.shape`**: Prints the shape (dimensions) of the training label data.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Loading the Testing Data**\n",
    "\n",
    "```python\n",
    "cifar_test = pd.read_csv('./test_data.csv')\n",
    "```\n",
    "\n",
    "- **`pd.read_csv('./test_data.csv')`**: Loads the CIFAR-10 testing dataset from the `test_data.csv` file into a Pandas DataFrame called `cifar_test`.\n",
    "\n",
    "```python\n",
    "x_test = cifar_test.drop('label', axis=1).values\n",
    "```\n",
    "\n",
    "- **`cifar_test.drop('label', axis=1)`**: Drops the `label` column from the testing DataFrame.\n",
    "- **`.values`**: Converts the testing feature data into a NumPy array.\n",
    "\n",
    "```python\n",
    "y_test = cifar_test['label'].values\n",
    "```\n",
    "\n",
    "- **`cifar_test['label']`**: Extracts the `label` column from the test DataFrame.\n",
    "- **`.values`**: Converts the test labels into a NumPy array.\n",
    "\n",
    "```python\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "```\n",
    "\n",
    "- **`x_test.shape`**: Prints the shape (dimensions) of the testing feature data.\n",
    "- **`y_test.shape`**: Prints the shape (dimensions) of the testing label data.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Normalizing the Data**\n",
    "\n",
    "```python\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "```\n",
    "\n",
    "- **`x_train / 255.0`**: Normalizes the pixel values of the training data by dividing by 255.0, as pixel values in images typically range from 0 to 255. This scales the values to a range of 0 to 1, which helps improve the neural network’s training process.\n",
    "- **`x_test / 255.0`**: Similarly, normalizes the pixel values of the testing data.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Reshaping the Data**\n",
    "\n",
    "```python\n",
    "x_train = x_train.reshape((-1, 32, 32, 3))\n",
    "x_test = x_test.reshape((-1, 32, 32, 3))\n",
    "```\n",
    "\n",
    "- **`.reshape((-1, 32, 32, 3))`**: Reshapes the data to match the input shape expected by the convolutional layers of the neural network:\n",
    "  - `-1`: Automatically calculates the number of samples.\n",
    "  - `32, 32`: Image dimensions (32x32 pixels).\n",
    "  - `3`: Number of channels (RGB channels, hence 3).\n",
    "  \n",
    "---\n",
    "\n",
    "### **6. Defining the Model**\n",
    "\n",
    "```python\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "- **`models.Sequential([...])`**: Creates a sequential model, where layers are stacked in a linear fashion.\n",
    "- **`layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3))`**: \n",
    "  - A 2D convolutional layer with 32 filters of size 3x3. \n",
    "  - **`activation='relu'`**: ReLU (Rectified Linear Unit) activation function is used to introduce non-linearity.\n",
    "  - **`input_shape=(32, 32, 3)`**: Specifies the input shape of each image (32x32 pixels, 3 color channels).\n",
    "- **`layers.MaxPooling2D((2, 2))`**: Max-pooling layer to downsample the image. It selects the maximum value \n",
    "from a 2x2 pool, reducing spatial dimensions.\n",
    "- **`layers.Conv2D(64, (3, 3), activation='relu')`**: Another convolutional layer with 64 filters and ReLU activation.\n",
    "- **`layers.MaxPooling2D((2, 2))`**: Another max-pooling layer.\n",
    "- **`layers.Conv2D(128, (3, 3), activation='relu')`**: Third convolutional layer with 128 filters and ReLU activation.\n",
    "- **`layers.MaxPooling2D((2, 2))`**: Another max-pooling layer.\n",
    "- **`layers.Flatten()`**: Flattens the multi-dimensional output from the convolutional layers into a 1D vector for the fully connected layers.\n",
    "- **`layers.Dense(128, activation='relu')`**: A fully connected (dense) layer with 128 units and ReLU activation.\n",
    "- **`layers.Dense(10, activation='softmax')`**: The output layer with 10 units (one for each class) and a `softmax` \n",
    "activation function, which is used for multi-class classification.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Compiling the Model**\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "- **`model.compile()`**: Configures the model for training.\n",
    "- **`optimizer='adam'`**: Uses the Adam optimizer, a popular optimization algorithm that adjusts the learning rate during training.\n",
    "- **`loss='sparse_categorical_crossentropy'`**: Specifies the loss function for multi-class classification where \n",
    "labels are not one-hot encoded (i.e., labels are integers).\n",
    "- **`metrics=['accuracy']`**: Tracks the accuracy metric during training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Training the Model**\n",
    "\n",
    "```python\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "```\n",
    "\n",
    "- **`model.fit()`**: Trains the model on the training data.\n",
    "- **`x_train, y_train`**: The training data and corresponding labels.\n",
    "- **`epochs=10`**: The number of times the entire training dataset is passed through the model.\n",
    "- **`validation_data=(x_test, y_test)`**: Provides the validation data to evaluate the model’s performance after each epoch.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Evaluating the Model**\n",
    "\n",
    "```python\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "```\n",
    "\n",
    "- **`model.evaluate(x_test, y_test)`**: Evaluates the model’s performance on the test data.\n",
    "- **`test_loss, test_acc`**: The loss and accuracy returned by the evaluation.\n",
    "- **`print(f'Test accuracy: {test_acc}')`**: Prints the test accuracy\n",
    "\n",
    " of the model.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Making Predictions**\n",
    "\n",
    "```python\n",
    "predictions = model.predict(x_test)\n",
    "```\n",
    "\n",
    "- **`model.predict(x_test)`**: Makes predictions on the test data using the trained model. The result is a set of probabilities for each class.\n",
    "\n",
    "```python\n",
    "n = 2\n",
    "plt.imshow(x_test[n].reshape(32, 32, 3))\n",
    "```\n",
    "\n",
    "- **`n = 2`**: Selects the 2nd image from the test set for visualization.\n",
    "- **`plt.imshow(x_test[n].reshape(32, 32, 3))`**: Reshapes the selected image and displays it using Matplotlib.\n",
    "\n",
    "```python\n",
    "print(\"Actual Label: \", classes[np.argmax(y_test[n])])\n",
    "print(\"Predicted Label: \", classes[np.argmax(predictions[n])])\n",
    "```\n",
    "\n",
    "- **`np.argmax(y_test[n])`**: Returns the index of the maximum value in the test label for the nth image, which corresponds to the actual class label.\n",
    "- **`np.argmax(predictions[n])`**: Returns the index of the maximum predicted value for the nth image.\n",
    "- **`classes[...]`**: Uses the index to find the actual and predicted class labels.\n",
    "\n",
    "---\n",
    "\n",
    "### **11. What is the role of `np.argmax()` in the code?**\n",
    "   **Answer:** `np.argmax()` returns the index of the largest value in an array, which corresponds to the class with the \n",
    "   highest predicted probability or the true class label.\n",
    "\n",
    "---\n",
    "\n",
    "This breakdown covers all major aspects of the code. Each line serves a specific function in the context of training a \n",
    "neural network to classify images from the CIFAR-10 dataset.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
